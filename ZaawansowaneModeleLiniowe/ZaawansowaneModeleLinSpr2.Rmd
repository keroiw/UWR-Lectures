---
title: "ZaawansowaneModeleLinSpr2"
knit: (function(input_file, encoding) {
  out_dir <- '../docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'Modele2.html'))})
author: "Błażej Wiórek"
date: "3/28/2020"
output: html_document
---

```{r setup, include=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(MASS)

knitr::opts_chunk$set(echo = TRUE)

set.seed(47)
```

## Zadanie 1

### Podpunkt a)

```{r data}
data <- structure(list(numeracy = c(6.6, 7.1, 7.3, 7.5, 7.9, 7.9, 8,
                                 8.2, 8.3, 8.3, 8.4, 8.4, 8.6, 8.7, 8.8, 8.8, 9.1, 9.1, 9.1, 9.3,
                                 9.5, 9.8, 10.1, 10.5, 10.6, 10.6, 10.6, 10.7, 10.8, 11, 11.1,
                                 11.2, 11.3, 12, 12.3, 12.4, 12.8, 12.8, 12.9, 13.4, 13.5, 13.6,
                                 13.8, 14.2, 14.3, 14.5, 14.6, 15, 15.1, 15.7), 
                    anxiety = c(13.8, 14.6, 17.4, 14.9, 13.4, 13.5, 13.8, 16.6, 13.5, 15.7, 13.6, 14,
                                16.1, 10.5, 16.9, 17.4, 13.9, 15.8, 16.4, 14.7, 15, 13.3, 10.9,
                                12.4, 12.9, 16.6, 16.9, 15.4, 13.1, 17.3, 13.1, 14, 17.7, 10.6,
                                14.7, 10.1, 11.6, 14.2, 12.1, 13.9, 11.4, 15.1, 13, 11.3, 11.4,
                                10.4, 14.4, 11, 14, 13.4), 
                    success = c(0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L,
                                1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 1L,
                                1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L)), 
                 .Names = c("numeracy", "anxiety", "success"), row.names = c(NA, -50L), class = "data.frame")
data_df <- as.data.frame(data)
```

Na podstawie twierdzenia podanego na wykładzie, estymator parametrów regresji logistycznej jest asymptotycznie normalny tzn:
$$
\hat{\beta} \sim N(\beta, X^T S(\beta) X)^{-1})
$$

Twierdzenie umożliwa obliczenie estymatora macierzy kowariancji wektora estymatorów w modelu regresji logistycznej. Wystarczy wstawić we wzorze na macierz kowariancji $\hat{\beta}$ zamiast $\beta$. Otrzymuje wtedy: $X^T S(\hat{\beta}) X)^{-1}$ 
```{r cov_mtrx}
model1 <- glm(success ~ numeracy + anxiety, data=data_df, family=binomial)
model1_summary <- summary(model1)
p <- model1$fitted.values
glm_cov_estimate <- vcov(model1)

X <- cbind(intercept=1, data_df)
X <- subset(X, select = -c(success))
X <- as.matrix(X)
W <- as.matrix(diag(p*(1-p)))

manual_cov_estimate <- solve(t(X) %*% W %*% X)
```

```{r glm_cov_estimate}
coefs_std  <- model1_summary$coefficients[, 2]
cov_estimate_std <- sqrt(diag(manual_cov_estimate))
std_comparison_df <- data.frame("GLM std" = coefs_std, "Cov std" = cov_estimate_std)
std_comparison_df
```

### Podpunkt b)

Chce przetestować hipotezę, że obie zmienne objaśniające nie mają wpływu na zmienną objaśnianą. Formalnie test mogę zapisać jako: $H0: \beta_{numeracy} = 0 \land \beta_{anxiety} = 0$ przy alternatywie $HA: \beta_{numeracy} \neq 0 \lor \beta_{anxiety} \neq 0$. Statystyka testowa ma postać: $\chi^2 = D(M1) - D(M2) \sim \chi^2_1$. $D(M1)$ oznacza deviance modelu w którym zachodzi $H0$, natomiast $D(M2)$ oznacza deviance modelu w którym zachodzi $HA$.

```{r chi_sq_test}
chi_sq_stat <- model1$null.deviance - model1$deviance
p_value <- pchisq(chi_sq_stat, df=1, lower.tail = FALSE)
print(paste("p-value for predictors importance: ", p_value))
```

Tak niska p-wartość daje podstawy do odrzucenia hipotezy zerowej na rzecz hipotezy alternatywnej. 

### Podpunkt c)

Nadmierna dyspersja to sytuacja w której wariancja założonego rozkładu jest mniejsza niż wariancja obserwowana w danych. W przypadku regresji logistycznej mogę mówić o nadmiernej dyspersji gdy obserwowana wariancja $y_i$ jest większa niż $Var[\hat{p}] = \frac{p(1-p)}{n}$. Istnieją dwie podstawowe przyczyny występowania tego zjawiska
1. Prawdopodobieństwo, że Y=1 zmienia się w kolejnych próbach
2. Prawdopodobieństwa w poszczególnych próbach nie spełniają warunku niezależności. 

Test na podstawie którego można wnioskować o nadmiernej dyspersji to test dopasowania modelu obliczany na podstawie statystyki Pearsona, residual deviance i liczby stopni swobody residual deviance.
```{r overdispersion_test}
overdispersion_stat <- sum(residuals(model1, type = "deviance")^2)
overdispersion_pval <- pchisq(overdispersion_stat, model1$df.residual, lower = F)
print(paste("P-value connected with test for overdispersion: ", overdispersion_pval))
```

Wysoka p-wartość wskazuje, że brak podstaw do stwierdzenia występowania zjawiska najdmiernej dyspersji. 

### Podpunkt d)

d) Podaj definicję parametru ‘’epsilon’’ i jego wartość domyślną. Wykonaj ponownie
obliczenia stosując wartości epsilon ze zbioru: 10^-1, 10^-2, 10^-3 i 10^-6. Porównaj
liczbę iteracji i wartości estymatorów poszczególnych parametrów.

Ze względu na to, że analityczne rozwiązanie równania $\nabla l(b) = 0$ (gdzie l(b) to funkcja likelihood względem parametrów modelu) nie istnieje, do obliczenia estymatorów współczynników modelu wykorzystywane są metody numeryczne. Parametr $\epsilon$ umożliwia kontrolę nad liczbą iteracji algorytmu. Zdefiniowany jest on jako $\frac{dev_{old} - dev}{|dev| + 0.1}$. Wartości występujące we wzorze to dewiancja aktualnego modelu oraz dewiancja modelu z poprzedniego kroku. Warto tutaj przypomnieć, że funkcja $l(b)$ jest funkcją wklęsłą co gwarantuje zbieżność algorytmu oraz $dev_{old} - dev \geq 0$. Wniosek jest taki, że zmniejszanie wartości $\epsilon$ spowoduje zwiększenie liczby iteracji ze względu na wymuszenie coraz dokładniejszej aproksymacji minimum globalnego. 

```{r epsilon}
epsilons <- c(1e-1, 1e-2, 1e-3, 1e-6)
model_e1 <- glm(success ~ numeracy + anxiety, data=data_df, family=binomial, epsilon=epsilons[1])
model_e2 <- glm(success ~ numeracy + anxiety, data=data_df, family=binomial, epsilon=epsilons[2])
model_e3 <- glm(success ~ numeracy + anxiety, data=data_df, family=binomial, epsilon=epsilons[3])
model_e4 <- glm(success ~ numeracy + anxiety, data=data_df, family=binomial, epsilon=epsilons[4])

models <- list(model_e1, model_e2, model_e3, model_e4)
epsilon_info_df <- as.data.frame(matrix(1:12, ncol=3))
colnames(epsilon_info_df) <- c("Iteration", "Epsilon", "Deviance")

for (i in 1:length(models)) {
  model <- models[[i]]
  epsilon_info_df[i, ] = c(model$iter, epsilons[i], model$dev)
}
epsilon_info_df
```


Zgodnie z oczekiwaniami zmniejszanie $\epsilon$ zwiększało liczbę iteracji oraz zmniejszało wartość statystyki deviance. Warto zauważyć, że różnice pomiędzy $\epsilon = 10^{-3}$ a $\epsilon = 10^{-6}$ są pomijalne. W praktyce dla dużych zbiorów danych zbyt duża liczba kroków algorytmu może w znaczący sposób wydłużyć czas uczenia. 

## Zadanie 2

Wygeneruj macierz X wymiaru n=400, p=3, której elementy są zmiennymi losowymi z rozkładu N(0, sigma^2=1/n). Załóżmy, że binarny wektor odpowiedzi jest wygenerowany zgodnie z modelem regresji logistycznej z wektorem beta=(3,3,3). Wyznacz macierz informacji
Fishera w punkcie beta i asymptotyczna macierz kowariancji estymatorów największej wiarogodności. Nastepnie 500 razy wygeneruj wektor odpowiedzi zgodnie z powyższym modelem. 

W związku z tym, że dane generowane są zgodnie z określonym schematem, model nie uwzględnia interceptu.
```{r assignment2_setup}
experiment_rep = 500

matrix_rnorm <- function(n, n_p) {
  X <- rnorm(n * n_p, sd=sqrt(1/n))
  X <- matrix(X, nrow=n)
}

cov_matrix <- function(n ,n_p) {
  mu <- rep(0, n_p)
  S <- matrix(0.3, nrow=3, ncol=3)
  diag(S) <- 1
  mvrnorm(n, mu, 1/n * S)
}

experiment <- function(n, n_p, X) {
  beta <- rep(3, n_p)
  logit <- X %*% beta
  p <- exp(logit) / (1 + exp(logit))
  
  S <- diag(as.vector(p*(1-p)))
  fisher_matrix <- t(X) %*% S %*% X
  cov_matrix <- solve(fisher_matrix)
  predictors <- c("beta1", "beta2", "beta3")
  colnames(cov_matrix) <- predictors
  rownames(cov_matrix) <- predictors

  collected_features <- c(predictors, "resid_dev")
  collected_features_n <- length(collected_features)
    
  estimates_df <- matrix(1:collected_features_n * experiment_rep, nrow=experiment_rep, ncol=collected_features_n)
  estimates_df <- as.data.frame(estimates_df)
  colnames(estimates_df) <- collected_features
  
  for (i in 1:experiment_rep) {
    Y <- rbinom(n, 1, p)
    model <- glm(Y~X - 1, family=binomial)
    estimates_df[i, 1:3] <- model$coefficients
    estimates_df[i, 4] <- model$dev
  }
  
  list(estimates=estimates_df, cov_matrix=cov_matrix)
}  

show_estimates_histogram <- function(estimates_df, cov_matrix, n) {
  models_info_long <- estimates_df %>% 
                      dplyr::select(starts_with("beta")) %>% 
                      gather(key='coefficient', value='coefficient_value')

  density_arg <- seq(min(models_info_long["coefficient_value"]), 
                     max(models_info_long["coefficient_value"]), 
                     by=0.1)
  coefs_density <- data.frame(beta1=dnorm(density_arg, mean=3, sd=sqrt(cov_matrix[1,1])),
                              beta2=dnorm(density_arg, mean=3, sd=sqrt(cov_matrix[2,2])),
                              beta3=dnorm(density_arg, mean=3, sd=sqrt(cov_matrix[3,3])))
  coefs_density_long <- coefs_density %>% gather(key='coefficient', value='density')
  coefs_density_long["args"] = rep(density_arg, 3)


  histogram <- ggplot(models_info_long, aes(x=coefficient_value)) + 
             geom_histogram(aes(y=..density..), colour="black", fill="white") + 
             geom_density(col="red") +
             geom_line(data=coefs_density_long, aes(x=args, y=density), col="blue") +         
             facet_grid(. ~ coefficient) +
             ggtitle(paste("Regression coefficient estimates n =", n))
  histogram
}

bias_estimation <- function(estimates_df) estimates_df %>% 
                                          dplyr::select(starts_with("beta")) %>% 
                                          mutate_all(function(x) {x-3}) %>% 
                                          colMeans()

variance_estimation <- function(estimates_df) estimates_df %>% 
                       dplyr::select(starts_with("beta")) %>% 
                       var()
```

### Podpunkt a)

Narysuj histogramy estymatorów beta1, beta2 i beta3 i ‘residual deviance’ i porównaj z ich rozkładami asymptotycznymi.

Zgodnie z twierdzeniem podanym na wykładzie wiem, że wektor estymtorów w regresji logistycznej jest asymptotycznie normalny:
$$
\hat{\beta} \xrightarrow{D} N(\beta, X^TS(\beta)X)
$$
przy czym $S(\beta)$ to macierz diagonalna mająca na przekątnej wartości postaci $p_i(1-p_i)$, $p_i = P(Y=1|X_i)$. Ze względu na schemat przeprowadzonego eksperymentu spodziewam się, że poszczególne współczynniki regresji otrzymane w kolejnych powtórzeniach powinny układać się zgodnie z rozkładem $N(3, 1)$. 

```{r histograms_coefficients_1}
estimates_res_400 <- experiment(400, 3, matrix_rnorm(400, 3))
estimates_df_400 <- estimates_res_400$estimates
cov_matrix_400 <- estimates_res_400$cov_matrix
histogram_400 <- show_estimates_histogram(estimates_df_400, cov_matrix_400, 400)
histogram_400
```

Niebieska linia na powyższym wykresie przedstawia gęstość rozkładu $N(3, 1)$. Można zaobserwować, że przybliżona linia gęstości rozkładu (kolor czerwony) reprezentująca ciągłą wersje histogramu odbiega w niewielkim stopniu od rozkładu asymptotycznego. 

### Podpunkt b)

b) Wyestymuj obciążenie estymatorów beta1, beta2 i beta3.

Obiążenie estymatorów obliczone zostało jako średnia różnica pomiędzy $\beta = 3$ a estymatorem $\hat{\beta}$. Jako, że $\hat{\beta}$ jest estymatorem nieobciążonym oczekuje się, że wartość ta będzie malała wraz ze wzrostem liczby obserwacji (n).

```{r bias_estimation_1}
estimated_bias_400 <- bias_estimation(estimates_df_400)
estimated_bias_400
```

### Podpunkt c)

c) Wyestymuj macierz kowariancji wektora estymatorów (beta1, beta2, beta3) i porównaj z
asymptotyczną macierzą kowariancji.

Wyestymwana macierz kowariancji:
```{r variance_estimated_1}
variance_estimation(estimates_df_400)
```

Prawdziwa macierz kowariancji:
```{r true_variance_1}
estimates_res_400$cov_matrix
```

Wartości 


## Zadanie 3

### Podpunkt a)

Zmniejszenie liczby obserwacji na podstawie których wystymowane są współczynniki regresji logistycznej powoduje zwiększenie błędy estymacji prawdziwych wartości współczynników $\beta$. Sytuacja ta jest w pewnym stopniu obserwowalna na poniższym zestawieniu histogamów, jednak zostanie uwypuklona w kolejnych podpunktach.

```{r histograms_coefficients_2}
estimates_res_100 <- experiment(100, 3, matrix_rnorm(100, 3))
estimates_df_100 <- estimates_res_100$estimates
cov_matrix_100 <- estimates_res_100$cov_matrix
histogram_100 <- show_estimates_histogram(estimates_df_100, cov_matrix_100, 100)

grid.arrange(histogram_100, histogram_400, nrow=2)
```

### Podpunkt b)


```{r bias_estimation_2}
estimated_bias_100 <- bias_estimation(estimates_df_100)
bias_summary <- dplyr::bind_rows(estimated_bias_400, estimated_bias_100) %>% as.data.frame()
rownames(bias_summary) = c("n=400", "n=100")
bias_summary
```

### Podpunkt c)

Wyestymwana macierz kowariancji:
```{r variance_estimated_2}
variance_estimation(estimates_df_100)
```

Prawdziwa macierz kowariancji:
```{r true_variance_2}
estimates_res_100$cov_matrix
```

### Zadanie 4

W przeciwieństwie do poprzedniego zadania, zbiór treningowy został wygenerowany w taki sposób, że istnieje korelacja pomiędzy poszczególnymi predyktorami. Zjawisko to znane jest jako multicollinearity i stanowi efekt niepożądany. 

## Podpunkt a) (n=400)

```{r histograms_coefficients_3}
estimates_res_400 <- experiment(400, 3, cov_matrix(400, 3))
estimates_df_400 <- estimates_res_400$estimates
cov_matrix_400 <- estimates_res_400$cov_matrix
histogram_400 <- show_estimates_histogram(estimates_df_400, cov_matrix_400, 400)
histogram_400
```

```{r bias_estimation_3}
estimated_bias_400_cov <- bias_estimation(estimates_df_400)
estimated_bias_400_cov
```

```{r variance_estimated_3}
variance_estimation(estimates_df_400)
```

```{r true_variance_3}
estimates_res_400$cov_matrix
```


## Podpunkt b) (n=100)


```{r histograms_coefficients_4}
estimates_res_100 <- experiment(100, 3, cov_matrix(100, 3))
estimates_df_100 <- estimates_res_100$estimates
cov_matrix_100 <- estimates_res_100$cov_matrix
histogram_100 <- show_estimates_histogram(estimates_df_100, cov_matrix_100, 100)

grid.arrange(histogram_100, histogram_400, nrow=2)
```

```{r bias_estimation_4}
estimated_bias_100_cov <- bias_estimation(estimates_df_100)
estimated_bias_100_cov
```

```{r variance_estimated_4}
variance_estimation(estimates_df_100)
```

```{r true_variance_4}
estimates_res_100$cov_matrix
```







