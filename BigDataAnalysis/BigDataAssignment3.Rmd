---
title: "Multiple testing procedures"
author: "Błażej Wiórek"
date: "5/13/2020"
output: pdf_document
---

```{r setup, include=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(nlme)
knitr::opts_chunk$set(echo = F)
```

## Multiple testing procedures

Sot far the only hypothesis that was considered was globall null $H_0 = \bigcap_i H_{0, i}$. The main problem was to maximize power while controlling probability of Type I error ($\alpha$). The problem with global test is that it doesn't provide information about single $H_{0, i}$. In order to address this issue multiple testing procedures were invited. In this case we are particullary interested in the value of unobserved random variable $V$ which counts the number of false discoveries. Instead of controlling $\alpha$ we are rather interested in controlling the following values:

* Family wise error rate: $FWER = \mathbb{P}(V \geq 1)$
* False discovery rate: $FDR = \mathbb{E} [ \frac{V}{max(R, 1)} ]$, where $R$ is the total amount of discoveries.

Family wise error rate works well when consequences of false discoveries are really serious. It may happen when there are relatively few hypotheses to test. In case when there are many hypotheses and we can acccept some amount of false discoveries, FDR provides compromise for Type I error - test power tradeoff.  It's immediate that controlling FWER causes control of FDR while converse is not true in general. Because FDR is based on average we are not able to make a claim about single experiment and there may be cases when FDR exceeds assumed threshold.

### Bonferroni method

The idea is to reject all $H_{0, i}$ for which $p_i \leq \frac{\alpha}{n}$. It turns out that Bonferroni method controls FWER at level $\alpha$ in strong sense:
$$
FWER \leq \mathbb{E}V = \frac{n_0}{n}\alpha \leq \alpha
$$

In above equation $n_0$ is amount of true null hypotheses. It's worth to notice that the level of control is the strongest when $n_0 = 0$ and decrease linearly as $n_0$ grows. 

### Benjamini-Hochberg procedure

Let's assume that p-values were ordered and $i_0$ is the largest $i$ for which $p_{(i)} \leq \frac{i}{n}q$. Then we reject all $H_{(i)}$ with $i \leq i_0$. 

**Theorem**: For independent p-values, the Benjamini-Hochberg procedure controls the FDR at level $\alpha$ which can be written as:
$$
FDR = \frac{n_0}{n} \alpha \leq \alpha
$$ 
There are two important properties that link FDR to FWER:

1. Under the globall null, the FDR is equivalent to the FWER.
1. $FWER \geq FDR$.


## Assignment 1

Consider a low dimensional setup: $p = 20$, for $i = \{1, . . . , 10\}$, $μ_i = 2 ∗ ln(20/i)$ and $\mu_{11} =...= \mu_{20} = 0$. Compare FWER, FDR and Power (proportion of identifed alternative hypotesis among all alternative hypotheses) of the Bonferroni and the Benjamini-Hochberg procedure.

Note: In order to estimate requested values all experiments in this report were repeated 10000 times. All tests were performed assuming significance level $\alpha = 0.05$.

```{r experiment_1}
mu <- sqrt(2 * log(20 / 1:10))
mu <- c(mu, rep(0, 10))

# FWER, FDR, power
rep <- 10000
alpha <- 0.05

power_test <- function(p, alpha, rng) sum(p[rng] < alpha) / length(rng)
fwer <- function(p, alpha, rng) sum(p[rng] < alpha) > 0
fdr <- function(p, alpha, rng) sum(p[rng] < alpha) / max(1, sum(p < alpha)) 

experiment <- function(mu, alpha, rng_idx) {
  mu_len <- length(mu)
  x <- rnorm(mu_len, mean=mu)
  p <- 2*pmin(pnorm(x), 1-pnorm(x))
  
  bonf_p <- p.adjust(p, method="bonferroni")
  bh_p <- p.adjust(p, method="BH")
  
  data.frame(b_fwer=fwer(bonf_p, alpha, (rng_idx+1):mu_len), 
             b_fdr=fdr(bonf_p, alpha, (rng_idx+1):mu_len), 
             b_power=power_test(bonf_p, alpha, 1:rng_idx), 
             bh_fwer=fwer(bh_p, alpha, (rng_idx+1):mu_len), 
             bh_fdr=fdr(bh_p, alpha, (rng_idx+1):mu_len), 
             bh_power=power_test(bh_p, alpha, 1:rng_idx))
}

df <- dplyr::bind_rows(replicate(rep, experiment(mu=mu, alpha=alpha, rng_idx=10), F))
summary_df1 <- t(as.data.frame(colMeans(df)))
round(summary_df1, 4)
```

Prefix b refers to Bonferroni procedure and prexi bh refers to Benjamini-Hochberg procedure.

### Bonferroni

* Bonferoni method controls FWER in a stronge sense, hence it's not surprising that habving $n_0=10$ the following results were obtained: $FWER \leq 0.25 = \frac{10}{20} 0.05 = \frac{n_0}{n} \alpha$.
* $FWER \geq FDR$ which satisfies properties of these values.
* Power of this meathod is low because strength of the signals is below threshol $\sqrt{2logp}$.

### Benjamini-Hochberg

* $FWER$ is fluctuating around assumed significance level $\alpha$, but it tends to be higher.
* As expected $FDR \leq \alpha$ and it's close to theoretic value $\frac{n_0}{n}\alpha$.
* Power of this procedure is significantly higher that the estimated power of Bonferroni procedure.

## Assignment 2

Large dimensional set-up. Let $p = 5000$ and

* $\mu_1=1.2\sqrt{2logp}$ , $\mu_2 = . . . = \mu_p = 0$
* $\mu_1= ... = \mu_{100} = 1.2\sqrt{2log(\frac{p}{100})}$ , $\mu_{101} = . . . = \mu_{5000} = 0$
* $\mu_1= ... = \mu_{1000} = 1.2\sqrt{2log(\frac{p}{1000})}$ , $\mu_{1001} = . . . = \mu_{5000} = 0$

In each of the above settings compare FWER, FDR and Power of the Bonferroni and the
Benjamini and Hochberg multiple testing procedures.

In the first case strength of the signals is above $\sqrt{2logp}$ threshold for Bonferroni testing procedure, hence it is expected that it will have full power. The second and third cases are slightly different. The number of signal is increased, but their strength is decreased. Additionally we are below $\sqrt{2logp}$ threshold. 

Summary table contains the results of all experiments: assignment 1 - exp1, assignment 2 a) - exp2 and so on. 
```{r assignment_2}
p <- 5000

### a) 

mu <- rep(0, p)
mu[1] <- 1.2 * sqrt(2 * log(p)) 

df1 <- dplyr::bind_rows(replicate(rep, experiment(mu=mu, alpha=alpha, rng_idx=1), F))
summary_df2 <- t(as.data.frame(colMeans(df1)))

### b)

mu <- rep(0, p)
mu[1:100] <- 1.2 * sqrt(2 * log(p/100))

df2 <- dplyr::bind_rows(replicate(rep, experiment(mu=mu, alpha=alpha, rng_idx=100), F))
summary_df3 <- t(as.data.frame(colMeans(df2)))

### c)

mu <- rep(0, p)
mu[1:1000] <- 1.2 * sqrt(2 * log(p/1000))

df3 <- dplyr::bind_rows(replicate(rep, experiment(mu=mu, alpha=alpha, rng_idx=1000), F))
summary_df4 <- t(as.data.frame(colMeans(df3)))

total_summary <- rbind(summary_df1, summary_df2, summary_df3, summary_df4)
rownames(total_summary)<-c("ex1", "ex2", "ex3", "ex4")
round(total_summary, 4)
```

The conclusions are generalization of insights that were already made in assignment 1.

### Bonferroni

* In all cases $FWER \leq alpha$ which presents strong control of probability occurrence of Type I error. 
* In all cases $FWER \geq FDR$ which satisfies properties of these values.
* Power of Bonferroni testing procedure is the highest for experiment 2 where the signal was above the threshold $\sqrt{2logp}$. Smaller values of the signals are immediately reflected in decrease of power, no matter their amount.  

### Benjamini-Hochberg

* It turns out that $FWER$ is the highest when there are many small signals (exp3 and exp4).
* In 3 out of 4 cases $FDR \leq \alpha$. When amount of tested hypotheses get's bigger, the deviatons from theoretic value $\frac{n_0}{n}\alpha$ grows as well.
* In all cases power of this procedure is significantly higher that the estimated power of Bonferroni procedure.












